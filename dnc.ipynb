{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKe1-QtnwfDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'MY ATTEMPT AT IMPLEMENTING Differentiable Neural Computer'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdFuLnZIwtzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'IMPORT LIBRARIES'\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "tf.compat.v1.set_random_seed(10)\n",
        "tf.compat.v1.reset_default_graph()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z4m4N25wyE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'HELPER METHODS'\n",
        "\n",
        "output_dim = 4\n",
        "hid_dim=32\n",
        "N = 10\n",
        "W = 10\n",
        "R = 1\n",
        "input_features = 50\n",
        "\n",
        "    \n",
        "def oneplus(x):\n",
        "    return 1 + tf.nn.softplus(x)\n",
        "    \n",
        "sizes = [R * W, R, W, 1, W, W, R, 1, 1]\n",
        "names = [\"r_read_keys\", \"r_read_strengths\", \"write_key\", \"write_strength\", \"erase_vector\", \"write_vector\",\n",
        "         \"r_free_gates\", \"allocation_gate\", \"write_gate\"]\n",
        "functions = [tf.identity, oneplus, tf.identity, oneplus, tf.nn.sigmoid, tf.identity,\n",
        "             tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid]\n",
        "indexes = [[sum(sizes[:i]), sum(sizes[:i + 1])] for i in range(len(sizes))]\n",
        "assert len(names) == len(sizes) == len(functions) == len(indexes)\n",
        "# Computing the interface dictionary from ordered lists of sizes, vector names and functions, as in the paper\n",
        "# self.split_interface is called later with the actual computed interface vector\n",
        "split_interface = lambda iv: {name: fn(iv[:, i[0]:i[1]]) for name, i, fn in\n",
        "                                   zip(names, indexes, functions)}\n",
        "\n",
        "epsilon = 1e-6\n",
        "epsilon = int(epsilon)\n",
        "\n",
        "def content_lookup(memory, keys, strength):\n",
        "    \"\"\"\n",
        "\n",
        "    :param memory: array of shape [batch_size, memory_size, word_size], i.e. [b, n, w]\n",
        "    :param keys: array of shape [batch_size, n_keys, word_size], i.e. [b, r, w]\n",
        "    :param strength: array of shape [batch_size, n_keys], i.e. [b, r]\n",
        "    :return: tensor of shape [batch_size, memory_size, n_keys]\n",
        "    \"\"\"\n",
        "    keys = tf.nn.l2_normalize(keys, axis=2)\n",
        "    memory = tf.nn.l2_normalize(memory, axis=2)\n",
        "    similarity = tf.einsum(\"bnw,brw,br->bnr\", memory, keys, strength)\n",
        "    content_weighting = tf.nn.softmax(similarity, axis=1, name=\"Content_weighting\")\n",
        "\n",
        "    return content_weighting\n",
        "\n",
        "def calculate_allocation_weighting(usage_vec):\n",
        "    usage_vector = epsilon + (1 - epsilon) * usage_vec\n",
        "\n",
        "    highest_usage, inverse_indices = tf.nn.top_k(-usage_vector, k=N*W)\n",
        "    lowest_usage = -highest_usage\n",
        "\n",
        "    allocation_scrambled = (1 - lowest_usage) * tf.math.cumprod(lowest_usage, axis=1, exclusive=True)\n",
        "\n",
        "    # allocation is not in the correct order. alloation[i] contains the sorted[i] value\n",
        "    # reversing the already inversed indices for each batch\n",
        "    indices = tf.stack([tf.math.invert_permutation(batch_indices) for batch_indices in tf.unstack(inverse_indices)])\n",
        "    allocation = tf.stack([tf.gather(mem, ind)\n",
        "                           for mem, ind in\n",
        "                           zip(tf.unstack(allocation_scrambled), tf.unstack(indices))])\n",
        "\n",
        "    return allocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9rLByLRxBge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'THE DNC'\n",
        "\n",
        "epsilon = 1e-6\n",
        "epsilon = int(epsilon)   \n",
        "class DNC: \n",
        "    def __init__(self, x, N, W, R, batch_size):\n",
        "      with tf.device('/GPU:0'):\n",
        "        self.x = x\n",
        "        print('X: ', self.x.shape)\n",
        "        self.N = N\n",
        "        print('N: ', self.N)\n",
        "        self.W = W\n",
        "        print('W: ', self.W)\n",
        "        self.R = R\n",
        "        print('R: ', self.R)\n",
        "        self.batch_size = batch_size\n",
        "        print('batch_size: ', self.batch_size)\n",
        "        self.memory_size = self.N*self.W\n",
        "        print('MEMORY SIZE: ', self.memory_size)\n",
        "        \n",
        "\n",
        "        self.memory = tf.fill([self.batch_size, self.N*self.W, self.W], epsilon)\n",
        "        print('MEMORY: ', self.memory)\n",
        "        self.memory = tf.cast(self.memory, tf.float32)\n",
        "\n",
        "        self.read_vectors = tf.fill([self.batch_size, self.R*self.W], epsilon)\n",
        "        self.read_vectors = tf.cast(self.read_vectors, tf.float32)\n",
        "        print('READ VECTORS: ', self.read_vectors.shape)\n",
        "        \n",
        "\n",
        "        self.input_dim = self.read_vectors.shape[-1] + input_features \n",
        "        print('FEATURES: ', input_features) \n",
        "        print('INPUT DIM: ', self.input_dim) #CONTROLLER INPUT DIMENSION IS SUM OF INPUT FEATURES AND READ VECTOR SIZE\n",
        "\n",
        "        # HIDDEN\n",
        "        self.w1 = tf.Variable(tf.compat.v1.random_normal([self.input_dim, hid_dim]), dtype=tf.float32)\n",
        "        self.b1 = tf.Variable(tf.compat.v1.random_normal([hid_dim]), dtype=tf.float32)\n",
        "\n",
        "        # OUT\n",
        "        self.w2 = tf.Variable(tf.compat.v1.random_normal([hid_dim, output_dim]), dtype=tf.float32)\n",
        "        self.b2 = tf.Variable(tf.compat.v1.random_normal([output_dim]), dtype=tf.float32)\n",
        "\n",
        "        self.interface_vec_size = (self.W*self.R) + 3*self.W + 5*self.R + 3\n",
        "        self.interface_weights = tf.Variable(tf.compat.v1.random_normal([hid_dim, self.interface_vec_size]), dtype=tf.float32)\n",
        "        print('INTERFACE WEIGHTS: ', self.interface_weights.shape)\n",
        "\n",
        "        self.memory_weights = tf.Variable(tf.compat.v1.random_normal([self.R* self.W, output_dim]))\n",
        "        print('MEMORY WEIGHTS: ', self.memory_weights.shape)\n",
        "\n",
        "        #initial interface weightings\n",
        "        self.write_weighting = tf.fill([self.batch_size, self.N*self.W], epsilon, name=\"Write_weighting\")\n",
        "        self.write_weighting = tf.cast(self.write_weighting, tf.float32)\n",
        "\n",
        "        self.usage_vector = tf.zeros([self.batch_size, self.N*self.W], name=\"Usage_vector\")\n",
        "\n",
        "        self.read_weightings = tf.fill([self.batch_size, self.N*self.W, self.R], epsilon)\n",
        "        self.read_weightings = tf.cast(self.read_weightings, tf.float32)\n",
        "        \n",
        "        self.sizes = [self.R * self.W, self.R, self.W, 1, self.W, self.W, self.R, 1, 1]\n",
        "        self.names = [\"r_read_keys\", \"r_read_strengths\", \"write_key\", \"write_strength\", \"erase_vector\", \"write_vector\",\n",
        "                 \"r_free_gates\", \"allocation_gate\", \"write_gate\"]\n",
        "        self.functions = [tf.identity, oneplus, tf.identity, oneplus, tf.nn.sigmoid, tf.identity,\n",
        "                     tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid]\n",
        "\n",
        "        self.indexes = [[sum(self.sizes[:i]), sum(self.sizes[:i + 1])] for i in range(len(self.sizes))]\n",
        "        assert len(self.names) == len(self.sizes) == len(self.functions) == len(self.indexes)\n",
        "        self.split_interface = lambda iv: {name: fn(iv[:, i[0]:i[1]]) for name, i, fn in\n",
        "                                           zip(self.names, self.indexes, self.functions)}\n",
        "\n",
        "        \n",
        "    \n",
        "    def step(self): \n",
        "      with tf.device('/GPU:0'):\n",
        "        global memory, write_weighting \n",
        "        self.x = tf.concat([self.x, self.read_vectors], axis=1) \n",
        "        print('X AFTER CONCATENATION: ', self.x.shape)\n",
        "        self.h1 = tf.nn.relu(tf.matmul(self.x, self.w1) + self.b1)\n",
        "        print('H1: ', self.h1.shape)\n",
        "        self.controller_out = tf.matmul(self.h1, self.w2) + self.b2\n",
        "        print('CONTROLLER OUTPUT: ', self.controller_out.shape)\n",
        "        self.interface_out = tf.matmul(self.h1, self.interface_weights)\n",
        "        print('INTERFACE OUTPUT: ', self.interface_out.shape)\n",
        "        self.intf = self.split_interface(self.interface_out)\n",
        "        #reshape keys\n",
        "        print('WRITE KEY: ', self.intf['write_key'].shape)\n",
        "        self.intf[\"write_key\"] = tf.expand_dims(self.intf[\"write_key\"], axis=1)\n",
        "        print('WRITE KEY: ', self.intf['write_key'].shape)\n",
        "        self.intf[\"r_read_keys\"] = tf.reshape(self.intf[\"r_read_keys\"], [-1, R, W])\n",
        "        #WRITE\n",
        "        self.write_content = content_lookup(self.memory, self.intf['write_key'], self.intf['write_strength'])\n",
        "        print('WRITE CONTENT: ', self.write_content.shape)\n",
        "        print('ALLOC GATE: ', self.intf[\"allocation_gate\"].shape)\n",
        "        # dynamic allocation\n",
        "        self.memory_retention = tf.reduce_prod(1 - tf.einsum(\"br,bnr->bnr\", self.intf[\"r_free_gates\"], self.read_weightings), 2)\n",
        "        self.usage_vector = (self.usage_vector + self.write_weighting - self.usage_vector * \n",
        "                             self.write_weighting) * self.memory_retention\n",
        "\n",
        "        self.write_allocation = calculate_allocation_weighting(self.usage_vector)\n",
        "        #final write calculations\n",
        "        print('WRITE WEIGHTING: ', self.write_weighting.shape)\n",
        "        print('WRITE ALLOCATION: ', self.write_allocation.shape)\n",
        "        print('WRITE GATE: ', self.intf['write_gate'].shape)\n",
        "        self.write_weighting = self.intf[\"write_gate\"] * (self.intf[\"allocation_gate\"] * self.write_allocation\n",
        "                                                      + tf.einsum(\"bnr,bi->bn\",\n",
        "                                                                  self.write_content,\n",
        "                                                                  (1 - self.intf[\"allocation_gate\"])))\n",
        "        \n",
        "        self.memory = self.memory * (1 - tf.einsum(\"bn,bw->bnw\", self.write_weighting, self.intf[\"erase_vector\"])) + \\\n",
        "                    tf.einsum(\"bn,bw->bnw\", self.write_weighting, self.intf[\"write_vector\"])\n",
        "\n",
        "        print(self.memory)\n",
        "        #\n",
        "        #READ \n",
        "        \n",
        "        self.read_content_lookup = content_lookup(self.memory, self.intf['r_read_keys'], self.intf['r_read_strengths'])\n",
        "        print('READ LOOK-UP WEIGHTS: ', self.read_content_lookup.shape)\n",
        "        self.read_weightings = self.read_content_lookup\n",
        "        print('READ WEIGHTING: ', self.read_weightings.shape)\n",
        "        self.current_read_vectors = tf.einsum(\"bnw,bnr->brw\", self.memory, self.read_weightings)\n",
        "        self.current_read_vectors = tf.reshape(self.current_read_vectors, (-1, self.W * self.R))\n",
        "        print('READ VECTORS: ', self.current_read_vectors.shape)\n",
        "        self.memory_output = tf.matmul(self.current_read_vectors, self.memory_weights)\n",
        "        print('MEMORY OUTPUT: ', self.memory_output.shape)\n",
        "\n",
        "        self.read_vectors = self.current_read_vectors\n",
        "        \n",
        "        #bypass dropout\n",
        "        #self.controller_out = tf.nn.dropout(self.controller_out, rate=0.5)\n",
        "\n",
        "        self.final_out = self.controller_out + self.memory_output  \n",
        "        print('FINAL OUTPUT: ', self.final_out.shape)\n",
        "        print(self.final_out)\n",
        "        return self.final_out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYX5uYvAxLmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'INSTANTIATE DNC'\n",
        "batch_size = 1\n",
        "features = 50\n",
        "\n",
        "input_dim_init = 50\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "\n",
        "  x = tf.compat.v1.placeholder(tf.float32, (batch_size, input_dim_init), name='x')\n",
        "  print('X: ', x.shape)\n",
        "  y = tf.compat.v1.placeholder(tf.int32, (batch_size, ), name='y')\n",
        "  print('Y: ', y.shape)\n",
        "\n",
        "  a = DNC(x, N, W, R, batch_size)\n",
        "\n",
        "  out = a.step()\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=out, labels=y))\n",
        "  optimizer = tf.compat.v1.train.AdamOptimizer(0.001).minimize(loss)\n",
        "  pred = tf.nn.softmax(out)\n",
        "\n",
        "  print(pred.shape)\n",
        "\n",
        "  correct_pred = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}